{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "YOLO : You Only Look Once (Real time object recognition algorithm)\n",
        "\n",
        "\n",
        "1. It divides the image into NxN grids\n",
        "1. Bounding box regression : Each grid cell is sent to the model. Then YOLO determines the probability of the cell contains a certain class and the class with the maximum probability is chosen.\n",
        "1. Intersection Over Union (IOU) : metric that evaluates intersection between the predicted bounding box and the ground truth bounding box. A Non-max suppression technique is applied to eliminate the bounding boxes that are very close by performing the IoU with the one having the highest class probability among them.\n",
        "\n",
        "\n",
        "    IOU = B1 intersection B2 / B1 union B2\n",
        "  \n",
        "\n",
        "\n",
        "We then get a class probability Map\n",
        "\n",
        "\n",
        "\n",
        "# Network Architecture\n",
        "- The YOLO network has 24 convolutional layers followed by 2 fully connected layers. The convolutional layers are pre-trained on the ImageNet classification task at half the resolution (224 × 224 input image) and then double the resolution for detection.\n",
        "- The layers Alternating 1 × 1 reduction layer and 3×3 convolutional layer to reduce the feature space from preceding layers.\n",
        "- The last 4 layers are added to train the network for object detection.\n",
        "- The last layer predicts the object class probability and the bounding box probability."
      ],
      "metadata": {
        "id": "Uh8bK2xe28t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV DNN Module\n",
        "\n",
        "Used to implement YOLOv3. \n",
        "\n",
        "Classify : \n",
        "- Cars\n",
        "- Heavy Motor Vehicles\n",
        "- Light Motor Vehicles\n",
        "\n",
        "\n",
        "Count the number of vehicles\n",
        "Store the data to analyse it\n"
      ],
      "metadata": {
        "id": "EC7BmvDV4blR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Modules \n",
        "# Install Python3\n",
        "\n",
        "! pip3 install opencv-python\n",
        "! pip3 install opencv-contrib-python # Running GPU Models\n",
        "! pip3 install numpy\n",
        "! pip3 install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvCuFDQP5iPP",
        "outputId": "70022f46-5fca-4422-e610-7720a284a90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Tracker File"
      ],
      "metadata": {
        "id": "KqybrJtn7h4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vehicle Counter\n"
      ],
      "metadata": {
        "id": "tZyM-nTx8P0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import cv2\n",
        "import csv\n",
        "import collections\n",
        "import numpy as np\n",
        "from tracker import *\n",
        "\n",
        "# Initialize Tracker\n",
        "tracker = EuclideanDistTracker()\n",
        "\n",
        "\n",
        "# Detection confidence threshold\n",
        "confThreshold = 0.2 # minimum confidence score threshold for detection\n",
        "nmsThreshold = 0.2 # Non-Max suppression threshold\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmvZeUn-8Rwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Middle cross line position\n",
        "# crossing line positions that will be used to count vehicles\n",
        "# (ToDo : Modify According to your need)\n",
        "middle_line_position = 225   \n",
        "up_line_position = middle_line_position - 15\n",
        "down_line_position = middle_line_position + 15"
      ],
      "metadata": {
        "id": "8ZOxWuJf969c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Coco Dataser Names in a list\n",
        "classesFile = \"coco.names\"\n",
        "classNames = open(classesFile).read().strip().split('\\n')\n",
        "print(classNames)\n",
        "print(len(classNames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA1W7OoR-Y2L",
        "outputId": "d927ade9-f6d5-4505-d7f6-8cd3bb075c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv3 is trained on the coco dataset, but we need only a few items\n",
        "# class index for our required detection classes\n",
        "required_class_index = [2, 3, 5, 7]\n",
        "\n",
        "detected_classNames = []"
      ],
      "metadata": {
        "id": "RxtRAAUK-7uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Files\n",
        "modelConfiguration = 'yolov3-320.cfg'\n",
        "modelWeigheights = 'yolov3-320.weights'\n",
        "\n",
        "# configure the network model\n",
        "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeigheights)\n",
        "\n",
        "# Configure the network backend\n",
        "# If we are using a GPU, else comment out these lines\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "\n",
        "# Define random colour for each class\n",
        "np.random.seed(32)\n",
        "colors = np.random.randint(0, 255, size=(len(classNames), 3), dtype='uint8') # Setting random colour for each class"
      ],
      "metadata": {
        "id": "8iasZWRW_TZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Frames from a video file\n",
        "\n",
        "# Initialize the videocapture object\n",
        "cap = cv2.VideoCapture('video.mp4') # VideoCapture Object\n",
        "input_size = 320\n",
        "\n",
        "\n",
        "font_color = (0, 0, 255)\n",
        "font_size = 0.5\n",
        "font_thickness = 2\n",
        "\n",
        "# List for store vehicle count information\n",
        "# Create two temporary empty lists to store the vehicles id’s that enter the entry crossing line.\n",
        "temp_up_list = []\n",
        "temp_down_list = []\n",
        "\n",
        "# for counting those 4 vehicle classes in the up route and down route.\n",
        "up_list = [0, 0, 0, 0]\n",
        "down_list = [0, 0, 0, 0]\n",
        "\n",
        "\n",
        "def realTime():\n",
        "    while True:\n",
        "        success, img = cap.read() # Read Each frame\n",
        "        img = cv2.resize(img,(0,0),None,0.5,0.5) # Reduce frames by 50%\n",
        "        ih, iw, channels = img.shape\n",
        "        blob = cv2.dnn.blobFromImage(img, 1 / 255, (input_size, input_size), [0, 0, 0], 1, crop=False)\n",
        "\n",
        "        # Set the input of the network\n",
        "        net.setInput(blob)\n",
        "        layersNames = net.getLayerNames()\n",
        "        outputNames = [(layersNames[i[0] - 1]) for i in net.getUnconnectedOutLayers()]\n",
        "        # Feed data to the network\n",
        "        outputs = net.forward(outputNames)\n",
        "    \n",
        "        # Find the objects from the network output\n",
        "        postProcess(outputs,img)\n",
        "\n",
        "        # Draw the crossing lines\n",
        "\n",
        "        cv2.line(img, (0, middle_line_position), (iw, middle_line_position), (255, 0, 255), 2)\n",
        "        cv2.line(img, (0, up_line_position), (iw, up_line_position), (0, 0, 255), 2)\n",
        "        cv2.line(img, (0, down_line_position), (iw, down_line_position), (0, 0, 255), 2)\n",
        "\n",
        "        # Draw counting texts in the frame\n",
        "        cv2.putText(img, \"Up\", (110, 20), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "        cv2.putText(img, \"Down\", (160, 20), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "        cv2.putText(img, \"Car:        \"+str(up_list[0])+\"     \"+ str(down_list[0]), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "        cv2.putText(img, \"Motorbike:  \"+str(up_list[1])+\"     \"+ str(down_list[1]), (20, 60), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "        cv2.putText(img, \"Bus:        \"+str(up_list[2])+\"     \"+ str(down_list[2]), (20, 80), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "        cv2.putText(img, \"Truck:      \"+str(up_list[3])+\"     \"+ str(down_list[3]), (20, 100), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "\n",
        "        # Show the frames\n",
        "        cv2.imshow('Output', img)\n",
        "\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Write the vehicle counting information in a file and save it\n",
        "    # DONE AT LAST\n",
        "\n",
        "    with open(\"data.csv\", 'w') as f1:\n",
        "        cwriter = csv.writer(f1)\n",
        "        cwriter.writerow(['Direction', 'car', 'motorbike', 'bus', 'truck'])\n",
        "        up_list.insert(0, \"Up\")\n",
        "        down_list.insert(0, \"Down\")\n",
        "        cwriter.writerow(up_list)\n",
        "        cwriter.writerow(down_list)\n",
        "    f1.close()\n",
        "    # print(\"Data saved at 'data.csv'\")\n",
        "    # Finally realese the capture object and destroy all active windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "CxDihZp0QDJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess frame and run the detection\n",
        "input_size = 320\n",
        "def from_static_image(image):\n",
        "    img = cv2.imread(image)\n",
        "\n",
        "    # Take image as input and return a resized and normalized BLOB Object\n",
        "    blob = cv2.dnn.blobFromImage(img, 1 / 255, (input_size, input_size), [0, 0, 0], 1, crop=False)\n",
        "\n",
        "    # Set the input of the network\n",
        "    net.setInput(blob)\n",
        "    layersNames = net.getLayerNames()\n",
        "    outputNames = [(layersNames[i[0] - 1]) for i in net.getUnconnectedOutLayers()]\n",
        "    # Feed data to the network\n",
        "    outputs = net.forward(outputNames)\n",
        "\n",
        "    # Find the objects from the network output / Custim Function\n",
        "    postProcess(outputs,img)\n",
        "\n",
        "    # count the frequency of detected classes\n",
        "    frequency = collections.Counter(detected_classNames)\n",
        "    print(frequency)\n",
        "    # Draw counting texts in the frame\n",
        "    cv2.putText(img, \"Car:        \"+str(frequency['car']), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "    cv2.putText(img, \"Motorbike:  \"+str(frequency['motorbike']), (20, 60), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "    cv2.putText(img, \"Bus:        \"+str(frequency['bus']), (20, 80), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "    cv2.putText(img, \"Truck:      \"+str(frequency['truck']), (20, 100), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_color, font_thickness)\n",
        "\n",
        "\n",
        "    cv2.imshow(\"image\", img)\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "    # save the data to a csv file\n",
        "    with open(\"static-data.csv\", 'a') as f1:\n",
        "        cwriter = csv.writer(f1)\n",
        "        cwriter.writerow([image, frequency['car'], frequency['motorbike'], frequency['bus'], frequency['truck']])\n",
        "    f1.close()"
      ],
      "metadata": {
        "id": "TAijjfNo48QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post Process the output data : \n",
        "# Function for finding the detected objects from the network output\n",
        "\n",
        "'''\n",
        "Network output has 3 vectors of length of 85 each \n",
        "4x the bounding box (centerX, centerY, width, height)\n",
        "1x box confidence\n",
        "80x class confidence\n",
        "'''\n",
        "\n",
        "def postProcess(outputs,img):\n",
        "    global detected_classNames # List to store all detected classes in a frame\n",
        "    height, width = img.shape[:2]\n",
        "    boxes = []\n",
        "    classIds = []\n",
        "    confidence_scores = []\n",
        "    detection = []\n",
        "    for output in outputs: # Iterate through each vector of each output and collect confidence scores for class index\n",
        "        for det in output:\n",
        "            scores = det[5:]\n",
        "            classId = np.argmax(scores)\n",
        "            confidence = scores[classId]\n",
        "            if classId in required_class_index:\n",
        "              #  if the class confidence score is greater than our defined confThreshold, then we collect the information about the class and store the box coordinate points, class-Id, and Confidence score in three separate lists.\n",
        "                if confidence > confThreshold:\n",
        "                    # print(classId)\n",
        "                    w,h = int(det[2]*width) , int(det[3]*height)\n",
        "                    x,y = int((det[0]*width)-w/2) , int((det[1]*height)-h/2)\n",
        "                    boxes.append([x,y,w,h])\n",
        "                    classIds.append(classId)\n",
        "                    confidence_scores.append(float(confidence))\n",
        "\n",
        "    # Apply Non-Max Suppression\n",
        "    # YOLO sometimes gives multiple bounding boxes for a single object, so we have to reduce the number of detection boxes and have to take the best detection box for each class.\n",
        "    # Reduce the number of boxes and take only the best box\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidence_scores, confThreshold, nmsThreshold)\n",
        "    for i in indices.flatten():\n",
        "        x, y, w, h = boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]\n",
        "        # print(x,y,w,h)\n",
        "\n",
        "        color = [int(c) for c in colors[classIds[i]]]\n",
        "        name = classNames[classIds[i]]\n",
        "        detected_classNames.append(name)\n",
        "        # Draw classname and confidence score  (Text in the frame)\n",
        "        cv2.putText(img,f'{name.upper()} {int(confidence_scores[i]*100)}%',\n",
        "                  (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
        "        detection.append([x, y, w, h, required_class_index.index(classIds[i])])\n",
        "\n",
        "    # Update the tracker for each object (Track and Count)\n",
        "    boxes_ids = tracker.update(detection)\n",
        "    for box_id in boxes_ids:\n",
        "        count_vehicle(box_id, img)\n"
      ],
      "metadata": {
        "id": "448dQ-Xt4-2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for finding the center of a rectangle\n",
        "# Used in Next Box\n",
        "def find_center(x, y, w, h):\n",
        "    x1=int(w/2)\n",
        "    y1=int(h/2)\n",
        "    cx = x+x1\n",
        "    cy=y+y1\n",
        "    return cx, cy"
      ],
      "metadata": {
        "id": "FMWhDz7D9Zdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for count vehicle\n",
        "def count_vehicle(box_id, img):\n",
        "\n",
        "    x, y, w, h, id, index = box_id\n",
        "\n",
        "    # Find the center of the rectangle for detection\n",
        "    center = find_center(x, y, w, h)\n",
        "    ix, iy = center\n",
        "    \n",
        "    # Find the current position of the vehicle\n",
        "    # Keep track of the current position and their corresponding IDs\n",
        "    if (iy > up_line_position) and (iy < middle_line_position):\n",
        "\n",
        "        if id not in temp_up_list:\n",
        "            temp_up_list.append(id)\n",
        "\n",
        "    elif iy < down_line_position and iy > middle_line_position:\n",
        "        if id not in temp_down_list:\n",
        "            temp_down_list.append(id)\n",
        "            \n",
        "    elif iy < up_line_position:\n",
        "        if id in temp_down_list:\n",
        "            temp_down_list.remove(id)\n",
        "            up_list[index] = up_list[index]+1\n",
        "\n",
        "    elif iy > down_line_position:\n",
        "        if id in temp_up_list:\n",
        "            temp_up_list.remove(id)\n",
        "            down_list[index] = down_list[index] + 1\n",
        "\n",
        "    # Draw circle in the middle of the rectangle\n",
        "    cv2.circle(img, center, 2, (0, 0, 255), -1)\n",
        "    # print(up_list, down_list)"
      ],
      "metadata": {
        "id": "BSgR147j9Xsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file = 'vehicle classification-image02.png'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    realTime()\n",
        "    # from_static_image(image_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "YcTOsITa-J34",
        "outputId": "57306a0c-b510-4677-ba32-6f0d867ae2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-290b846197b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrealTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# from_static_image(image_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a7eca743a403>\u001b[0m in \u001b[0;36mrealTime\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlayersNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayersNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Feed data to the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a7eca743a403>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlayersNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayersNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Feed data to the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    }
  ]
}